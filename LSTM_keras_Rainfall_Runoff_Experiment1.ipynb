{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_keras_Rainfall_Runoff_Experiment1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "xreWcmRRb4sL"
      ],
      "authorship_tag": "ABX9TyOJIaMttZQPpZAFBXSXeoDN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JapeTheEternalChild/LSTM-Streamflowprediction-with-Keras/blob/main/LSTM_keras_Rainfall_Runoff_Experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UMkoc204LZx"
      },
      "source": [
        "# Rainfall-runoff modelling using Long Short-Term Memory (LSTM) networks (Library:Keras)\n",
        "##Experiment 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnvbzluF4k_5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgawZ5Gm5JdE"
      },
      "source": [
        "## Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbTCnx-J5HR7"
      },
      "source": [
        "# import libraries\n",
        "from pydrive.drive import GoogleDrive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "import os\n",
        "import glob\n",
        "# load keras and co\n",
        "import keras\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import CuDNNLSTM\n",
        "from keras import backend as K\n",
        "import torch\n",
        "# from sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FTFljv65fvD"
      },
      "source": [
        "## mount google drive & GPU connect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xDthHfa0tcn",
        "outputId": "8b21b2e3-68a9-4883-b3d8-c233efc663b2"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqhnL1e15Yxg",
        "outputId": "8412336f-22e7-4357-ec50-5e2f332245f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX_kePZd4lf7"
      },
      "source": [
        "## 1. Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTfQBT7O4mt2"
      },
      "source": [
        "# Import & Preprocess Data Function\n",
        "\n",
        "def read_data(filename_daymet, filename_flow):\n",
        "  # daily metereo mean data\n",
        "  daymet = pd.read_csv(filename_daymet, sep='\\t', header=4)\n",
        "  daymet.columns =['Datetime', 'dayl(s)', 'prcp(mm/d)', 'srad(W/m2)', 'swe(mm)', 'tmax(c)', 'tmin(C)', 'vp(Pa)']  # name columns\n",
        "  # get catchment area\n",
        "  with open(filename_daymet, 'r') as fp:\n",
        "      content = fp.readlines()\n",
        "      area = int(content[2])\n",
        "  # daily streamflow data\n",
        "  flow = pd.read_csv(filename_flow, header=None, delim_whitespace=True)\n",
        "  flow = flow[4] # select column\n",
        "  flow = flow[1:] # drop first line 1.1.1980 because daymet starts from the 2.1.1980\n",
        "  flow.reset_index(drop=True, inplace=True) # reset index\n",
        "  flow = 28316846.592 * flow * 86400 / (area * 10 ** 6) # from cubic feet per second to equivalent water column in mm\n",
        "  # combine daymet and streamflow\n",
        "  data = pd.concat([daymet, flow], axis=1)\n",
        "  data['Datetime'] =  pd.to_datetime(data['Datetime'], format='%Y %m %d %H')  # set Datetime format\n",
        "  data['Datetime'] = pd.to_datetime(data['Datetime']).dt.date # drop hours\n",
        "  data = data.set_index(pd.DatetimeIndex(data['Datetime']))\n",
        "  data = data.drop(['swe(mm)'], axis=1)\n",
        "  data = data.drop(['Datetime'], axis=1)\n",
        "  data = data.drop(['dayl(s)'], axis=1)\n",
        "  data_old = data.shape[0]\n",
        "  # delete missing data\n",
        "  \n",
        "  data.columns =['prcp(mm/d)', 'srad(W/m2)', 'tmax(c)', 'tmin(C)', 'vp(Pa)', 'Q_spec(mm)']  # name columns\n",
        "  idxNegatives = data[data['Q_spec(mm)'] < 0].index \n",
        "  data.drop(idxNegatives, inplace = True)\n",
        "  data.dropna(inplace = True)\n",
        "  # Print how many samples where deleted\n",
        "  deleted = data_old-data.shape[0]\n",
        "  print( deleted, 'SAMPLES WHERE DELETED DUE TO MISSING DATA')\n",
        "\n",
        "  return data\n",
        "\n",
        "# split data\n",
        "#define training and test periods\n",
        "def split_data1(data):\n",
        "  df_train = data['1980-10-01':'1995-09-30']\n",
        "  df_test = data['1995-10-01':'2010-09-30']\n",
        "  return df_train, df_test\n",
        "\n",
        "# Z-Standartization --> scaling of data (mean=0, standard deviation = 1)\n",
        "\n",
        "def local_standartization(data):\n",
        "  stds = data.std()\n",
        "  mean = data.mean()\n",
        "  scaled_data = (data-mean)/stds\n",
        "  return scaled_data, stds, mean\n",
        "\n",
        "def scale(data, stds, mean):\n",
        "  scaled_data = (data-mean)/stds\n",
        "  return scaled_data\n",
        "\n",
        "# Rescaling discharge values \n",
        "\n",
        "def rescale(data_discharge, stds, mean):\n",
        "  y_stds = stds.loc['Q_spec(mm)']\n",
        "  y_mean = mean.loc['Q_spec(mm)']\n",
        "  rescaled_discharge = (data_discharge * y_stds) +  y_mean\n",
        "  return rescaled_discharge\n",
        "\n",
        "# shift timeseries for t days\n",
        "\n",
        "def shift(data, t):\n",
        "  data_to_shift = data.drop(['Q_spec(mm)'], axis = 1)\n",
        "  Q = data['Q_spec(mm)']\n",
        "  lags= range(0,t)\n",
        "  data_shifted = pd.concat([data_to_shift.shift(t).add_suffix(f\" (t-{t})\") for t in lags], axis=1)\n",
        "  data_shifted = pd.concat([data_shifted, Q], axis = 1)\n",
        "  return data_shifted\n",
        "\n",
        "# split dataframe into train and test data; define input(x) and control(y) values; reshape input df from 2D to 3D\n",
        "\n",
        "def split_data(train_shifted, test_shifted):\n",
        "  #df_train = train_shifted['1980-10-01':'1995-09-30']\n",
        "  #df_test = test_shifted['1995-10-01':'2010-09-30']\n",
        "  y_train = train_shifted['Q_spec(mm)'].values\n",
        "  x_train = train_shifted.loc[:, train_shifted.columns != 'Q_spec(mm)'].values\n",
        "  y_test = test_shifted['Q_spec(mm)'].values\n",
        "  x_test = test_shifted.loc[:, test_shifted.columns != 'Q_spec(mm)'].values\n",
        "  #x_train = np.expand_dims(x_train,2)\n",
        "  #x_test = np.expand_dims(x_test, 2)\n",
        "  return y_train, x_train, y_test, x_test\n",
        "\n",
        "#NSE function\n",
        "def get_nse(y_test, predictions):\n",
        "  numerator = sum([(y_test[i]-predictions[i])**2 for i in range(len(y_test))])\n",
        "  y_test_avg = sum(y_test)/len(y_test)\n",
        "  denominator = sum([(y_test[i]-y_test_avg)**2 for i in range(len(y_test))])\n",
        "  NSE = 1- (numerator/denominator)\n",
        "  return NSE\n",
        "\n",
        "def calc_nse(obs: np.array, sim: np.array) -> float:\n",
        "    \"\"\"Calculate Nash-Sutcliff-Efficiency.\n",
        "\n",
        "    :param obs: Array containing the observations\n",
        "    :param sim: Array containing the simulations\n",
        "    :return: NSE value.\n",
        "    \"\"\"\n",
        "    # only consider time steps, where observations are available\n",
        "    sim = np.delete(sim, np.argwhere(obs < 0), axis=0)\n",
        "    obs = np.delete(obs, np.argwhere(obs < 0), axis=0)\n",
        "\n",
        "    # check for NaNs in observations\n",
        "    sim = np.delete(sim, np.argwhere(np.isnan(obs)), axis=0)\n",
        "    obs = np.delete(obs, np.argwhere(np.isnan(obs)), axis=0)\n",
        "\n",
        "    denominator = np.sum((obs - np.mean(obs)) ** 2)\n",
        "    numerator = np.sum((sim - obs) ** 2)\n",
        "    nse_val = 1 - numerator / denominator\n",
        "\n",
        "    return nse_val\n",
        "\n",
        "#using NSE as metrics function\n",
        "def metrics_nse(y_obs,y_sim):\n",
        "  numerator = K.sum(K.square(y_sim - y_obs))\n",
        "  denominator = K.sum(K.square(y_obs - K.mean(y_obs)))\n",
        "  return 1-(numerator/denominator)\n",
        "\n",
        "#using NSE as loss function\n",
        "def loss_NSE(y_obs, y_sim):\n",
        "  numerator = K.sum(K.square(y_sim - y_obs))\n",
        "  denominator = K.sum(K.square(y_obs - K.mean(y_obs)))\n",
        "  return numerator/denominator\n",
        "\n",
        "#LSTM\n",
        "\n",
        "def model_lstm(x_train, loss, metrics):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(20, input_shape=x_train.shape[1:], return_sequences = True, activation = \"tanh\"))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(LSTM(20, activation = \"tanh\"))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss=loss, optimizer='adam', metrics=metrics)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "# optimizer\n",
        "\n",
        "\n",
        "\n",
        "# GRU\n",
        "''' gated recurrent unit after Cho, et al. in 2014\n",
        "using a keras based sequencial model '''\n",
        "\n",
        "def model_gru(x_train, loss, metrics):\n",
        "  model = Sequential()\n",
        "  model.add(GRU(20, activation=\"tanh\", input_shape=x_train.shape[1:], use_bias=True, return_sequences = True, recurrent_activation=\"sigmoid\"))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(GRU(20, activation=\"tanh\", use_bias=True, recurrent_activation=\"sigmoid\"))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss=loss, optimizer=opt, metrics=metrics)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "# Train the Model\n",
        "\n",
        "def train(model, x_train, y_train, epochs, batch_size):\n",
        " history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
        " return history\n",
        "\n",
        " # Evaluate model\n",
        "\n",
        "def get_accuracy(model,x,y):\n",
        " loss = model.evaluate(x, y)\n",
        " return loss\n",
        "\n",
        "# Validation\n",
        "\n",
        "def prediction(model, x_test):\n",
        "  predictions = model.predict(x_test)\n",
        "  return predictions\n",
        "\n",
        "def lstm_data_transform(x_data, y_data, num_steps=5):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "# Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "# Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "# if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "# Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "# Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "# Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "class CustomCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    y_pred = self.model.predict(self.validation_data[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDtpehs3u55G"
      },
      "source": [
        "If validation loss >> training loss you can call it overfitting.\n",
        "\n",
        "If validation loss  > training loss you can call it some overfitting.\n",
        "\n",
        "If validation loss  < training loss you can call it some underfitting.\n",
        "\n",
        "If validation loss << training loss you can call it underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FOfd62eJUu6"
      },
      "source": [
        "## Routine implementation\n",
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmvIGC9KJbBT"
      },
      "source": [
        "RNN='LSTM'\n",
        "\n",
        "### Specify HUC of interest ###\n",
        "HUC_ID = '1'\n",
        "# Define path for daymet\n",
        "start_path_daymet = '/content/drive/My Drive/Colab Notebooks/basin_mean_forcing/daymet'\n",
        "final_path_daymet = os.path.join(start_path_daymet, HUC_ID, '*.txt')\n",
        "#define path for streamflow\n",
        "start_path_flow = '/content/drive/My Drive/Colab Notebooks/usgs_streamflow'\n",
        "final_path_flow = os.path.join(start_path_flow, HUC_ID, '*.txt')\n",
        "# Create List with all filenames (all catchments in HUC)\n",
        "catchments_daymet = []\n",
        "catchments_flow = []\n",
        "for file in glob.glob(final_path_daymet):\n",
        "    catchments_daymet.append(file)\n",
        "for file in glob.glob(final_path_flow):\n",
        "  catchments_flow.append(file)\n",
        "# Create empty list to store model results in\n",
        "df_result = []\n",
        "\n",
        "### METRICS\n",
        "loss = 'mse'\n",
        "metrics = metrics_nse\n",
        "### HYPERPARAMETER ###\n",
        "t = 365 #lag time(days) input\n",
        "epochs = 50\n",
        "batch_size = 512\n",
        "\n",
        "############# START OF LOOPING THE MODEL FOR ALL CATCHMENTS IN HUC #############\n",
        "for i in range(len(catchments_daymet)):\n",
        "\n",
        "  print('--------Loading Data...........................................................')\n",
        "  # get path\n",
        "  filename_daymet = catchments_daymet[i]\n",
        "  # get catchmennt ID\n",
        "  catchment_ID = os.path.basename(filename_daymet).split('_', 1)[0]\n",
        "  #get path\n",
        "  filename_flow = [s for s in catchments_flow if catchment_ID in s]\n",
        "  # load data\n",
        "  data = read_data(filename_daymet, filename_flow[0])\n",
        "  print('--------Data-loading complete--------------------------------------------------')\n",
        "  print('--------Creating Training and Test samples.....................................')\n",
        "  # Split Train and Test Data\n",
        "  df_train, df_test = split_data1(data)\n",
        "  # Standardize data\n",
        "  ############ Question how to standardize: [a]=by using statistical values of training set (best when stats of both sets do not differ), [b]=by using training stats for training set and testing stats for test set, [c]=by using complete set stats (worst)\n",
        "  print('--------Standardizing data.....................................................')\n",
        "  scaled_train, train_std, train_mean = local_standartization(df_train)\n",
        "  scaled_test = scale(df_test, train_std, train_mean)\n",
        "  # split data\n",
        "  y_train_sc, x_train_sc, y_test_sc, x_test_sc = split_data(scaled_train, scaled_test)\n",
        "  # create sequences\n",
        "  print('--------Shaping sequences......................................................')\n",
        "  num_steps = 365\n",
        "  # training set\n",
        "  (x_train_transformed,\n",
        "  y_train_transformed) = lstm_data_transform(x_train_sc, y_train_sc, num_steps=num_steps)\n",
        "  assert x_train_transformed.shape[0] == y_train_transformed.shape[0]\n",
        "  # test set\n",
        "  (x_test_transformed,\n",
        "  y_test_transformed) = lstm_data_transform(x_test_sc, y_test_sc, num_steps=num_steps)\n",
        "  assert x_test_transformed.shape[0] == y_test_transformed.shape[0]\n",
        "  # initate and compile model, \n",
        "  model = model_lstm(x_train_transformed, loss, metrics)\n",
        "  print('--------START TRAINING OF CATCHMENT '+catchment_ID+' in HUC '+HUC_ID+'--------')\n",
        "  #print('--------Training period: 1981/10/01 - 1995/09/30------------------------------')\n",
        "  # train model\n",
        "  model.fit(x_train_transformed, y_train_transformed, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True)\n",
        "  # accuracy\n",
        "  #accuracy = get_accuracy(model,x_train_transformed,y_train_transformed)\n",
        "  print('--------End of Training-------------------------------------------------------')\n",
        "  print('--------Start of Evaluation---------------------------------------------------')\n",
        "  #print('--------Evaluation period : '+df_test.ix[0]+'-'+df_test.ix[-1]+'-------------------------------')\n",
        "  # validation\n",
        "  predictions= prediction(model, x_test_transformed)\n",
        "  train_output = prediction(model, x_train_transformed)\n",
        "  nse_val = get_nse(y_test_transformed, predictions)\n",
        "  nse_train = get_nse(y_train_transformed, train_output)\n",
        "  print('Training NSE = ', nse_train)\n",
        "  print('Validation NSE =', nse_val)\n",
        "  #print('Validation accuracy (NSE) = ' +nse_val+'-----')\n",
        "  #store paramters \n",
        "  df_result.append([catchment_ID, nse_train, nse_val])\n",
        "  #print predicitons \n",
        "  # create dfs containing model output and observed values (features, and targets)\n",
        "  pred_rescaled = rescale(predictions, train_std, train_mean)\n",
        "  train_rescaled = rescale(train_output, train_std, train_mean)\n",
        "  df_test_short = df_test.iloc[365:,:]\n",
        "  df_test_short['predictions']= pred_rescaled\n",
        "  df_train_short = df_train.iloc[365:,:]\n",
        "  df_train_short['predictions']= train_rescaled\n",
        "  #df_test = df_test['1996-01-02':'2010-01-01']\n",
        "  # plot example\n",
        "  #fig=plt.figure(figsize=(7, 5), dpi= 150, facecolor='w', edgecolor='k')\n",
        "  #plt.plot(df_test.index, df_test['Q_spec(mm)'])\n",
        "  #plt.ylabel('discharge Q_spec(mm)')\n",
        "  #plt.plot(df_test.index, pred_rescaled.flatten(), alpha=0.5)\n",
        "  #plt.show()\n",
        "  # save weights\n",
        "  os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment1/'+RNN+'/loss_MSE/weights')\n",
        "  model.save_weights(catchment_ID+'_model.h5')\n",
        "  # save model input and output dataframes\n",
        "  os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment1/'+RNN+'/loss_MSE/model_predictions')\n",
        "  df_test_short.to_csv(catchment_ID+'_test_pred.csv')\n",
        "  df_train_short.to_csv(catchment_ID+'_train_pred.csv')\n",
        "\n",
        "#initialise dataframe\n",
        "df_result = pd.DataFrame(df_result, columns=['catchment_ID', 'nse_train', 'nse_val'])\n",
        "#define current directory\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment1/'+RNN+'/loss_MSE/results')\n",
        "#save dataframe with results\n",
        "df_result.to_csv(HUC_ID+RNN+'_results.csv')\n",
        "# Show results\n",
        "print(df_result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Routine implementation\n",
        "GRU\n"
      ],
      "metadata": {
        "id": "KB3T5EJYar2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### \n",
        "\n",
        "RNN = \"GRU\"\n",
        "\n",
        "### Specify HUC of interest ###\n",
        "HUC_ID = '17'\n",
        "# Define path for daymet\n",
        "start_path_daymet = '/content/drive/My Drive/Colab Notebooks/basin_mean_forcing/daymet'\n",
        "final_path_daymet = os.path.join(start_path_daymet, HUC_ID, '*.txt')\n",
        "#define path for streamflow\n",
        "start_path_flow = '/content/drive/My Drive/Colab Notebooks/usgs_streamflow'\n",
        "final_path_flow = os.path.join(start_path_flow, HUC_ID, '*.txt')\n",
        "# Create List with all filenames (all catchments in HUC)\n",
        "catchments_daymet = []\n",
        "catchments_flow = []\n",
        "for file in glob.glob(final_path_daymet):\n",
        "    catchments_daymet.append(file)\n",
        "for file in glob.glob(final_path_flow):\n",
        "  catchments_flow.append(file)\n",
        "# Create empty list to store model results in\n",
        "df_result = []\n",
        "\n",
        "### METRICS\n",
        "loss = 'mse'\n",
        "metrics = metrics_nse\n",
        "### HYPERPARAMETER ###\n",
        "t = 365 #lag time(days) input\n",
        "epochs = 50\n",
        "batch_size = 512\n",
        "\n",
        "############# START OF LOOPING THE MODEL FOR ALL CATCHMENTS IN HUC #############\n",
        "for i in range(len(catchments_daymet)):\n",
        "\n",
        "  print('--------Loading Data...........................................................')\n",
        "  # get path\n",
        "  filename_daymet = catchments_daymet[i]\n",
        "  # get catchmennt ID\n",
        "  catchment_ID = os.path.basename(filename_daymet).split('_', 1)[0]\n",
        "  #get path\n",
        "  filename_flow = [s for s in catchments_flow if catchment_ID in s]\n",
        "  # load data\n",
        "  data = read_data(filename_daymet, filename_flow[0])\n",
        "  print('--------Data-loading complete--------------------------------------------------')\n",
        "  print('--------Creating Training and Test samples.....................................')\n",
        "  # Split Train and Test Data\n",
        "  df_train, df_test = split_data1(data)\n",
        "  # Standardize data\n",
        "  ############ Question how to standardize: [a]=by using statistical values of training set (best when stats of both sets do not differ), [b]=by using training stats for training set and testing stats for test set, [c]=by using complete set stats (worst)\n",
        "  print('--------Standardizing data.....................................................')\n",
        "  scaled_train, train_std, train_mean = local_standartization(df_train)\n",
        "  scaled_test = scale(df_test, train_std, train_mean)\n",
        "  # split data\n",
        "  y_train_sc, x_train_sc, y_test_sc, x_test_sc = split_data(scaled_train, scaled_test)\n",
        "  # create sequences\n",
        "  print('--------Shaping sequences......................................................')\n",
        "  num_steps = 365\n",
        "  # training set\n",
        "  (x_train_transformed,\n",
        "  y_train_transformed) = lstm_data_transform(x_train_sc, y_train_sc, num_steps=num_steps)\n",
        "  assert x_train_transformed.shape[0] == y_train_transformed.shape[0]\n",
        "  # test set\n",
        "  (x_test_transformed,\n",
        "  y_test_transformed) = lstm_data_transform(x_test_sc, y_test_sc, num_steps=num_steps)\n",
        "  assert x_test_transformed.shape[0] == y_test_transformed.shape[0]\n",
        "  # initate and compile model, \n",
        "  model = model_gru(x_train_transformed, loss, metrics)\n",
        "  print('--------START TRAINING OF CATCHMENT '+catchment_ID+' in HUC '+HUC_ID+'--------')\n",
        "  print('--------Training period: 1981/10/01 - 1995/09/30------------------------------')\n",
        "  # train model\n",
        "  model.fit(x_train_transformed, y_train_transformed, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True)\n",
        "  # accuracy\n",
        "  #accuracy = get_accuracy(model,x_train_transformed,y_train_transformed)\n",
        "  print('--------End of Training-------------------------------------------------------')\n",
        "  print('--------Start of Evaluation---------------------------------------------------')\n",
        "  print('--------Evaluation period 1996/10/01-2010/09/30-------------------------------')\n",
        "  # validation\n",
        "  predictions= prediction(model, x_test_transformed)\n",
        "  train_output = prediction(model, x_train_transformed)\n",
        "  nse_val = get_nse(y_test_transformed, predictions)\n",
        "  nse_train = get_nse(y_train_transformed, train_output)\n",
        "  print('Training NSE = ', nse_train)\n",
        "  print('Validation NSE =', nse_val)\n",
        "  #print('Validation accuracy (NSE) = ' +nse_val+'-----')\n",
        "  #store paramters \n",
        "  df_result.append([catchment_ID, nse_train, nse_val])\n",
        "  #print predicitons \n",
        "  #pred_rescaled = rescale(predictions, train_mean, train_std)\n",
        "  #df_test = df_test['1996-01-02':'2010-01-01']\n",
        "  # plot example\n",
        "  #fig=plt.figure(figsize=(7, 5), dpi= 150, facecolor='w', edgecolor='k')\n",
        "  #plt.plot(df_test.index, df_test['Q_spec(mm)'])\n",
        "  #plt.ylabel('discharge Q_spec(mm)')\n",
        "  #plt.plot(df_test.index, pred_rescaled.flatten(), alpha=0.5)\n",
        "  #plt.show()\n",
        "  # save weights\n",
        "  os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment1/'+RNN+'/loss_MSE')\n",
        "  model.save_weights(HUC_ID+'_'+catchment_ID+RNN+'_model.h5')\n",
        "\n",
        "#initialise dataframe\n",
        "df_result = pd.DataFrame(df_result, columns=['catchment_ID', 'nse_train', 'nse_val'])\n",
        "#define current directory\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment1/'+RNN+'/loss_MSE')\n",
        "#save dataframe with results\n",
        "df_result.to_csv(HUC_ID+RNN+'_results.csv')\n",
        "# Show results\n",
        "print(df_result)\n"
      ],
      "metadata": {
        "id": "zgm0FBxQaw3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}