{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_keras_Rainfall_Runoff_Experiment2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPMdawt2V6+n/OJ+cEseNGb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JapeTheEternalChild/LSTM-Streamflowprediction-with-Keras/blob/main/LSTM_keras_Rainfall_Runoff_Experiment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_8nPuAbt9k4"
      },
      "source": [
        "# Rainfall-runoff modelling using Long Short-Term Memory (LSTM) networks (Library:Keras)\n",
        "##Experiment 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08SmiGHduJaj"
      },
      "source": [
        "All catchments are part of bigger hydrological units (HUCs). they correspond to geographic areas that represent the drainage area of a major river or several combined drainage areas of smaller ones.\n",
        "\n",
        "In this experiment the samples of all catchments in a HUC serve as input data for the training of the lstm/gru, which increases the the all available training data many fold and thus gives the network more input to learn on. \n",
        "\n",
        "The validation then happens individually on each catchment with the gained weigths from the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhOSlnZuuIVF"
      },
      "source": [
        "# import libraries\n",
        "from pydrive.drive import GoogleDrive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "import os\n",
        "import glob\n",
        "# load keras and co\n",
        "import keras\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import CuDNNLSTM\n",
        "from keras import backend as K\n",
        "# from sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZREYRvgKuPyK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### functions\n"
      ],
      "metadata": {
        "id": "9VDuFLV1eUSI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWgeBAH7uS7a"
      },
      "source": [
        "# Import & Preprocess Data Function\n",
        "\n",
        "def read_data(filename_daymet, filename_flow):\n",
        "  print('loading data')\n",
        "  # daily metereo mean data\n",
        "  daymet = pd.read_csv(filename_daymet, sep='\\t', header=4)\n",
        "  daymet.columns =['Datetime', 'dayl(s)', 'prcp(mm/d)', 'srad(W/m2)', 'swe(mm)', 'tmax(c)', 'tmin(C)', 'vp(Pa)']  # name columns\n",
        "  # get catchment area\n",
        "  with open(filename_daymet, 'r') as fp:\n",
        "      content = fp.readlines()\n",
        "      area = int(content[2])\n",
        "  # daily streamflow data\n",
        "  flow = pd.read_csv(filename_flow, header=None, delim_whitespace=True)\n",
        "  flow = flow[4] # select column\n",
        "  flow = flow[1:] # drop first line 1.1.1980 because daymet starts from the 2.1.1980\n",
        "  flow.reset_index(drop=True, inplace=True) # reset index\n",
        "  flow = 28316846.592 * flow * 86400 / (area * 10 ** 6) # from cubic feet per second to equivalent water column in mm\n",
        "  # combine daymet and streamflow\n",
        "  data = pd.concat([daymet, flow], axis=1)\n",
        "  data['Datetime'] =  pd.to_datetime(data['Datetime'], format='%Y %m %d %H')  # set Datetime format\n",
        "  data['Datetime'] = pd.to_datetime(data['Datetime']).dt.date # drop hours\n",
        "  data = data.set_index(pd.DatetimeIndex(data['Datetime']))\n",
        "  data = data.drop(['swe(mm)'], axis=1)\n",
        "  data = data.drop(['Datetime'], axis=1)\n",
        "  data = data.drop(['dayl(s)'], axis=1)\n",
        "  data_old = data.shape[0]\n",
        "  # delete missing data\n",
        "  \n",
        "  data.columns =['prcp(mm/d)', 'srad(W/m2)', 'tmax(c)', 'tmin(C)', 'vp(Pa)', 'Q_spec(mm)']  # name columns\n",
        "  idxNegatives = data[data['Q_spec(mm)'] < 0].index \n",
        "  data.drop(idxNegatives, inplace = True)\n",
        "  data.dropna(inplace = True)\n",
        "  # Print how many samples where deleted\n",
        "  deleted = data_old-data.shape[0]\n",
        "  print( deleted, 'SAMPLES WHERE DELETED DUE TO MISSING DATA')\n",
        "\n",
        "  return data\n",
        "\n",
        "# split data\n",
        "\n",
        "def split_data1(data):\n",
        "  df_train = data['1980-10-01':'1995-09-30']\n",
        "  df_test = data['1995-10-01':'2010-09-30']\n",
        "  return df_train, df_test\n",
        "\n",
        "# Z-Standartization --> scaling of data (mean=0, standard deviation = 1)\n",
        "\n",
        "def local_standartization(data):\n",
        "  stds = data.std()\n",
        "  mean = data.mean()\n",
        "  scaled_data = (data-mean)/stds\n",
        "  return scaled_data, stds, mean\n",
        "\n",
        "def scale(data, stds, mean):\n",
        "  scaled_data = (data-mean)/stds\n",
        "  return scaled_data\n",
        "\n",
        "\n",
        "# Rescaling discharge values \n",
        "\n",
        "def rescale(data_discharge, stds, mean):\n",
        "  y_stds = stds.loc['Q_spec(mm)']\n",
        "  y_mean = mean.loc['Q_spec(mm)']\n",
        "  rescaled_discharge = (data_discharge * y_stds) +  y_mean\n",
        "  return rescaled_discharge\n",
        "\n",
        "# shift timeseries for t days\n",
        "\n",
        "def shift(data, t):\n",
        "  data_to_shift = data.drop(['Q_spec(mm)'], axis = 1)\n",
        "  Q = data['Q_spec(mm)']\n",
        "  lags= range(0,t)\n",
        "  data_shifted = pd.concat([data_to_shift.shift(t).add_suffix(f\" (t-{t})\") for t in lags], axis=1)\n",
        "  data_shifted = pd.concat([data_shifted, Q], axis = 1)\n",
        "  return data_shifted\n",
        "\n",
        "# split dataframe into train and test data; define input(x) and control(y) values; reshape input df from 2D to 3D\n",
        "\n",
        "#def split_data(train_shifted, test_shifted):\n",
        "#  df_train = train_shifted['1981-10-01':'1995-09-30']\n",
        "#  df_test = test_shifted['1995-10-01':'2010-09-30']\n",
        "#  y_train = df_train['Q_spec(mm)'].values\n",
        "#  x_train = df_train.loc[:, df_train.columns != 'Q_spec(mm)'].values\n",
        "#  y_test = df_test['Q_spec(mm)'].values\n",
        "#  x_test = df_test.loc[:, df_test.columns != 'Q_spec(mm)'].values\n",
        "  #x_train = np.expand_dims(x_train,2)\n",
        "  #x_test = np.expand_dims(x_test, 2)\n",
        "#  return y_train, x_train, y_test, x_test\n",
        "\n",
        "#NSE function\n",
        "def get_nse(y_test, predictions):\n",
        "  numerator = sum([(y_test[i]-predictions[i])**2 for i in range(len(y_test))])\n",
        "  y_test_avg = sum(y_test)/len(y_test)\n",
        "  denominator = sum([(y_test[i]-y_test_avg)**2 for i in range(len(y_test))])\n",
        "  NSE = 1- (numerator/denominator)\n",
        "  return NSE\n",
        "\n",
        "#using NSE as metrics function\n",
        "def metrics_nse(y_obs,y_sim):\n",
        "  numerator = K.sum(K.square(y_sim - y_obs))\n",
        "  denominator = K.sum(K.square(y_obs - K.mean(y_obs)))\n",
        "  return 1-(numerator/denominator)\n",
        "\n",
        "#using NSE as loss function\n",
        "def loss_NSE(y_obs, y_sim):\n",
        "  numerator = K.sum(K.square(y_sim - y_obs))\n",
        "  denominator = K.sum(K.square(y_obs - K.mean(y_obs)))\n",
        "  return numerator/denominator\n",
        "\n",
        "#LSTM\n",
        "\n",
        "def model_lstm(x_train, loss, metrics):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(20, input_shape=x_train.shape[1:], return_sequences = True, activation = \"tanh\"))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(LSTM(20, activation = \"tanh\"))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss=loss, optimizer='adam', metrics=metrics)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "# GRU\n",
        "''' gated recurrent unit after Cho, et al. in 2014\n",
        "using a keras based sequencial model '''\n",
        "\n",
        "def model_gru(x_train, loss, metrics):\n",
        "  model = Sequential()\n",
        "  model.add(GRU(20, activation=\"tanh\", input_shape=x_train.shape[1:], use_bias=True, return_sequences = True, recurrent_activation=\"sigmoid\"))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(GRU(20, activation=\"tanh\", use_bias=True, recurrent_activation=\"sigmoid\"))\n",
        "  model.add(Dropout(0.1))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss=loss, optimizer='adam', metrics=metrics)\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "# Train the Model\n",
        "\n",
        "def train(model, x_train, y_train, epochs, batch_size):\n",
        " history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True)\n",
        " return history\n",
        "\n",
        " # Evaluate model\n",
        "\n",
        "def get_accuracy(model,x,y):\n",
        " loss = model.evaluate(x, y)\n",
        " return loss\n",
        "\n",
        "# Validation\n",
        "\n",
        "def prediction(model, x_test):\n",
        "  predictions = model.predict(x_test)\n",
        "  return predictions\n",
        "\n",
        "def lstm_data_transform(x_data, y_data, num_steps):\n",
        "    \"\"\" Changes data to the format for LSTM training \n",
        "for sliding window approach \"\"\"\n",
        "# Prepare the list for the transformed data\n",
        "    X, y = list(), list()\n",
        "# Loop of the entire data set\n",
        "    for i in range(x_data.shape[0]):\n",
        "        # compute a new (sliding window) index\n",
        "        end_ix = i + num_steps\n",
        "# if index is larger than the size of the dataset, we stop\n",
        "        if end_ix >= x_data.shape[0]:\n",
        "            break\n",
        "# Get a sequence of data for x\n",
        "        seq_X = x_data[i:end_ix]\n",
        "        # Get only the last element of the sequency for y\n",
        "        seq_y = y_data[end_ix]\n",
        "# Append the list with sequencies\n",
        "        X.append(seq_X)\n",
        "        y.append(seq_y)\n",
        "# Make final arrays\n",
        "    x_array = np.array(X)\n",
        "    y_array = np.array(y)\n",
        "    return x_array, y_array\n",
        "\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "class CustomCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    y_pred = self.model.predict(self.validation_data[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM\n"
      ],
      "metadata": {
        "id": "SmpYGTBeeQaz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvdElORaMQO-"
      },
      "source": [
        "RNN = 'LSTM'\n",
        "\n",
        "### Specify HUC of interest ###\n",
        "HUC_ID = '17'\n",
        "# Define path for daymet\n",
        "start_path_daymet = '/content/drive/My Drive/Colab Notebooks/basin_mean_forcing/daymet'\n",
        "final_path_daymet = os.path.join(start_path_daymet, HUC_ID, '*.txt')\n",
        "#define path for streamflow\n",
        "start_path_flow = '/content/drive/My Drive/Colab Notebooks/usgs_streamflow'\n",
        "final_path_flow = os.path.join(start_path_flow, HUC_ID, '*.txt')\n",
        "# Create List with all filenames (all catchments in HUC)\n",
        "catchments_daymet = []\n",
        "catchments_flow = []\n",
        "for file in glob.glob(final_path_daymet):\n",
        "    catchments_daymet.append(file)\n",
        "for file in glob.glob(final_path_flow):\n",
        "  catchments_flow.append(file)\n",
        "# Create empty list to store model results in\n",
        "df_result = []\n",
        "\n",
        "df_train_list = []\n",
        "df_test_list = []\n",
        "catchment_ID_list = []\n",
        "num_samples = []\n",
        "\n",
        "### METRICS\n",
        "loss = 'mse'\n",
        "metrics = metrics_nse\n",
        "### HYPERPARAMETER ###\n",
        "num_steps = 365 #lag time(days) input\n",
        "epochs = 20\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "# Collect all dataframes and split them each into a test and a train df; aggregate the train dfs into one df\n",
        "for i in range(len(catchments_daymet)):\n",
        "  # get path\n",
        "  filename_daymet = catchments_daymet[i]\n",
        "  # get catchmennt ID\n",
        "  catchment_ID = os.path.basename(filename_daymet).split('_', 1)[0]\n",
        "  print('catchment:', catchment_ID)\n",
        "  #get path\n",
        "  filename_flow = [s for s in catchments_flow if catchment_ID in s]\n",
        "  # load data\n",
        "  data = read_data(filename_daymet, filename_flow[0])\n",
        "  # Split Train and Test Data\n",
        "  df_train, df_test = split_data1(data)\n",
        "  #save data\n",
        "  num_samples.append(len(df_train))\n",
        "  df_train_list.append(df_train)\n",
        "  df_test_list.append(df_test)\n",
        "  catchment_ID_list.append(catchment_ID)\n",
        "\n",
        "# preprocess training data\n",
        "df_train_complete = pd.concat(df_train_list)\n",
        "df_train_complete_scaled, train_std, train_mean = local_standartization(df_train_complete)\n",
        "\n",
        "idx_start=0\n",
        "df_train_scaled_list = []\n",
        "\n",
        "\n",
        "for i in range(len(num_samples)):\n",
        "  idx_end = idx_start+num_samples[i]\n",
        "  df_train_scaled = df_train_complete_scaled[idx_start:idx_end]\n",
        "  idx_start = idx_end\n",
        "  df_train_scaled_list.append(df_train_scaled)\n",
        "\n",
        "x_train_transformed_list = []\n",
        "y_train_transformed_list = []\n",
        "\n",
        "for i in range(len(df_train_scaled_list)):\n",
        "  # seperate input & target\n",
        "  df_train_scaled = df_train_scaled_list[i]\n",
        "  x_train = df_train_scaled.loc[:, df_train_scaled.columns != 'Q_spec(mm)'].values\n",
        "  y_train = df_train_scaled[['Q_spec(mm)']].values\n",
        "  #transform input \n",
        "  (x_train_transformed,y_train_transformed) = lstm_data_transform(x_train, y_train, num_steps=num_steps)\n",
        "  x_train_transformed_list.append(x_train_transformed)\n",
        "  y_train_transformed_list.append(y_train_transformed)\n",
        "\n",
        "x_train_transformed_complete = np.concatenate(x_train_transformed_list)\n",
        "y_train_transformed_complete = np.concatenate(y_train_transformed_list)\n",
        "\n",
        "del x_train_transformed_list\n",
        "del y_train_transformed_list\n",
        "\n",
        "#LSTM model initiation & training\n",
        "model = model_lstm(x_train_transformed_complete, loss, metrics)\n",
        "model.fit(x_train_transformed_complete, y_train_transformed_complete, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True)\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment2/'+RNN+'/loss_MSE/weights')\n",
        "model.save_weights(HUC_ID+'_model.h5')\n",
        "\n",
        "\n",
        "train_output = prediction(model, x_train_transformed_complete)\n",
        "nse_train = get_nse(y_train_transformed_complete, train_output)\n",
        "\n",
        "# Testing model on all catchments\n",
        "df_result = []\n",
        "\n",
        "for i in range(len(df_test_list)):\n",
        "  # get catchment ID and test datatframe\n",
        "  catchment_ID = catchment_ID_list[i]\n",
        "  df_test = df_test_list[i]\n",
        "  #scale df with train stats\n",
        "  df_test_scaled = scale(df_test, train_std, train_mean)\n",
        "  #split df into input and target\n",
        "  x_test = df_test_scaled.loc[:, df_test_scaled.columns != 'Q_spec(mm)']\n",
        "  y_test = df_test_scaled['Q_spec(mm)']\n",
        "  #time lag   \n",
        "  (x_test_transformed,y_test_transformed) = lstm_data_transform(x_test, y_test, num_steps=num_steps)\n",
        "  assert x_test_transformed.shape[0] == y_test_transformed.shape[0]\n",
        "  #prediction\n",
        "  predictions= prediction(model, x_test_transformed)\n",
        "  pred_rescaled = rescale(predictions, train_std, train_mean)\n",
        "  nse_val = get_nse(y_test_transformed, predictions)\n",
        "  df_test_short = df_test.iloc[365:,:]\n",
        "  df_test_short['predictions']= pred_rescaled\n",
        "  # save model input and output dataframes\n",
        "  os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment2/'+RNN+'/loss_MSE/model_predictions')\n",
        "  df_test_short.to_csv(catchment_ID+'_test_pred.csv')\n",
        "  #store paramters \n",
        "  df_result.append([catchment_ID, nse_train, nse_val])\n",
        "\n",
        "#initialise dataframe\n",
        "df_result = pd.DataFrame(df_result, columns=['catchment_ID', 'nse_train', 'nse_val'])\n",
        "df_result.head()\n",
        "#define current directory\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment2/'+RNN+'/loss_MSE/results')\n",
        "#save dataframe with results\n",
        "df_result.to_csv(HUC_ID+RNN+'_results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU\n"
      ],
      "metadata": {
        "id": "E9DtDwmxdWsH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0JNd9JDJAjA"
      },
      "source": [
        "### \n",
        "\n",
        "RNN = \"GRU\"\n",
        "\n",
        "\n",
        "### Specify HUC of interest ###\n",
        "HUC_ID = '01'\n",
        "# Define path for daymet\n",
        "start_path_daymet = '/content/drive/My Drive/Colab Notebooks/basin_mean_forcing/daymet'\n",
        "final_path_daymet = os.path.join(start_path_daymet, HUC_ID, '*.txt')\n",
        "#define path for streamflow\n",
        "start_path_flow = '/content/drive/My Drive/Colab Notebooks/usgs_streamflow'\n",
        "final_path_flow = os.path.join(start_path_flow, HUC_ID, '*.txt')\n",
        "# Create List with all filenames (all catchments in HUC)\n",
        "catchments_daymet = []\n",
        "catchments_flow = []\n",
        "for file in glob.glob(final_path_daymet):\n",
        "    catchments_daymet.append(file)\n",
        "for file in glob.glob(final_path_flow):\n",
        "  catchments_flow.append(file)\n",
        "# Create empty list to store model results in\n",
        "df_result = []\n",
        "\n",
        "df_train_list = []\n",
        "df_test_list = []\n",
        "catchment_ID_list = []\n",
        "num_samples = []\n",
        "\n",
        "### METRICS\n",
        "loss = 'mse'\n",
        "metrics = metrics_nse\n",
        "### HYPERPARAMETER ###\n",
        "num_steps = 365 #lag time(days) input\n",
        "epochs = 20\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "# Collect all dataframes and split them each into a test and a train df; aggregate the train dfs into one df\n",
        "for i in range(len(catchments_daymet)):\n",
        "  # get path\n",
        "  filename_daymet = catchments_daymet[i]\n",
        "  # get catchmennt ID\n",
        "  catchment_ID = os.path.basename(filename_daymet).split('_', 1)[0]\n",
        "  print('catchment:', catchment_ID)\n",
        "  #get path\n",
        "  filename_flow = [s for s in catchments_flow if catchment_ID in s]\n",
        "  # load data\n",
        "  data = read_data(filename_daymet, filename_flow[0])\n",
        "  # Split Train and Test Data\n",
        "  df_train, df_test = split_data1(data)\n",
        "  #save data\n",
        "  num_samples.append(len(df_train))\n",
        "  df_train_list.append(df_train)\n",
        "  df_test_list.append(df_test)\n",
        "  catchment_ID_list.append(catchment_ID)\n",
        "\n",
        "# preprocess training data\n",
        "df_train_complete = pd.concat(df_train_list)\n",
        "df_train_complete_scaled, train_std, train_mean = local_standartization(df_train_complete)\n",
        "\n",
        "idx_start=0\n",
        "df_train_scaled_list = []\n",
        "\n",
        "\n",
        "for i in range(len(num_samples)):\n",
        "  idx_end = idx_start+num_samples[i]\n",
        "  df_train_scaled = df_train_complete_scaled[idx_start:idx_end]\n",
        "  idx_start = idx_end\n",
        "  df_train_scaled_list.append(df_train_scaled)\n",
        "\n",
        "x_train_transformed_list = []\n",
        "y_train_transformed_list = []\n",
        "\n",
        "for i in range(len(df_train_scaled_list)):\n",
        "  # seperate input & target\n",
        "  df_train_scaled = df_train_scaled_list[i]\n",
        "  x_train = df_train_scaled.loc[:, df_train_scaled.columns != 'Q_spec(mm)'].values\n",
        "  y_train = df_train_scaled[['Q_spec(mm)']].values\n",
        "  #transform input \n",
        "  (x_train_transformed,y_train_transformed) = lstm_data_transform(x_train, y_train, num_steps=num_steps)\n",
        "  x_train_transformed_list.append(x_train_transformed)\n",
        "  y_train_transformed_list.append(y_train_transformed)\n",
        "\n",
        "x_train_transformed_complete = np.concatenate(x_train_transformed_list)\n",
        "y_train_transformed_complete = np.concatenate(y_train_transformed_list)\n",
        "\n",
        "del x_train_transformed_list\n",
        "del y_train_transformed_list\n",
        "\n",
        "#LSTM model initiation & training\n",
        "model = model_gru(x_train_transformed_complete, loss, metrics)\n",
        "model.fit(x_train_transformed_complete, y_train_transformed_complete, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=True)\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment2/GRU')\n",
        "model.save_weights(HUC_ID+RNN+'_model.h5')\n",
        "\n",
        "\n",
        "train_output = prediction(model, x_train_transformed_complete)\n",
        "nse_train = get_nse(y_train_transformed_complete, train_output)\n",
        "\n",
        "# Testing model on all catchments\n",
        "df_result = []\n",
        "\n",
        "for i in range(len(df_test_list)):\n",
        "  # get catchment ID and test datatframe\n",
        "  catchment_ID = catchment_ID_list[i]\n",
        "  df_test = df_test_list[i]\n",
        "  #scale df with train stats\n",
        "  df_test_scaled = scale(df_test, train_std, train_mean)\n",
        "  #split df into input and target\n",
        "  x_test = df_test_scaled.loc[:, df_test_scaled.columns != 'Q_spec(mm)']\n",
        "  y_test = df_test_scaled['Q_spec(mm)']\n",
        "  #time lag   \n",
        "  (x_test_transformed,y_test_transformed) = lstm_data_transform(x_test, y_test, num_steps=num_steps)\n",
        "  assert x_test_transformed.shape[0] == y_test_transformed.shape[0]\n",
        "  #prediction\n",
        "  predictions= prediction(model, x_test_transformed)\n",
        "  nse_val = get_nse(y_test_transformed, predictions)  \n",
        "  #store paramters \n",
        "  df_result.append([catchment_ID, nse_train, nse_val])\n",
        "\n",
        "#initialise dataframe\n",
        "df_result = pd.DataFrame(df_result, columns=['catchment_ID', 'nse_train', 'nse_val'])\n",
        "df_result.head()\n",
        "#define current directory\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/Experiment2/GRU')\n",
        "#save dataframe with results\n",
        "df_result.to_csv(HUC_ID+RNN+'_results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}